{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import gym\n",
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "import matplotlib.cm as cm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(model,self).__init__()\n",
    "        self.hidden_size=512\n",
    "        self.rnn= nn.RNN(input_size=4, hidden_size=512,num_layers=2,batch_first=True)\n",
    "        self.fc = nn.Linear(512, 2)\n",
    "        \n",
    "    def init_hidden(self,batch_size):\n",
    "        return (torch.zeros(2,batch_size, self.hidden_size))\n",
    "    \n",
    "    def forward(self,x,hidden):\n",
    "        x=x.reshape(x.shape[0],1,4)\n",
    "        x,h_0=self.rnn(x,hidden)\n",
    "        return self.fc(x.contiguous().view(x.size(0), -1))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00646086, -0.00111536,  0.00539625,  0.04000238])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0').unwrapped\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy=model()\n",
    "target_net=model()\n",
    "target_net.load_state_dict(policy.state_dict())\n",
    "target_net.eval()\n",
    "optimizer = optim.RMSprop(policy.parameters())\n",
    "criterion = F.smooth_l1_loss\n",
    "memory=10000\n",
    "store=[[dict()] for i in range(memory)]\n",
    "gamma=0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addEpisode(ind,prev,curr,reward,act):\n",
    "    if len(store[ind]) ==0:\n",
    "        store[ind][0]={'prev':prev,'curr':curr,'reward':reward,'action':act}\n",
    "    else:\n",
    "        store[ind].append({'prev':prev,'curr':curr,'reward':reward,'action':act})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNet(total_episodes):\n",
    "    if total_episodes==0:\n",
    "        return\n",
    "    ep=random.randint(0,total_episodes-1)\n",
    "    if len(store[ep]) < 8:\n",
    "        return\n",
    "    else:  \n",
    "        start=random.randint(1,len(store[ep])-1)\n",
    "        length=len(store[ep])\n",
    "        inp=[]\n",
    "        target=[]\n",
    "        rew=torch.Tensor(1,length-start)\n",
    "        actions=torch.Tensor(1,length-start)\n",
    "        \n",
    "        for i in range(start,length,1):\n",
    "            inp.append((store[ep][i]).get('prev'))\n",
    "            target.append((store[ep][i]).get('curr'))\n",
    "            rew[0][i-start]=store[ep][i].get('reward')\n",
    "            actions[0][i-start]=store[ep][i].get('action')\n",
    "        targets = torch.Tensor(target[0].shape[0],target[0].shape[1],target[0].shape[2])\n",
    "        torch.cat(target, out=targets)\n",
    "        ccs=torch.Tensor(inp[0].shape[0],inp[0].shape[1],inp[0].shape[2])\n",
    "        torch.cat(inp, out=ccs)\n",
    "        hidden = policy.init_hidden(length-start)\n",
    "        qvals= target_net(targets,hidden)\n",
    "        actions=actions.type('torch.LongTensor')\n",
    "        actions=actions.reshape(length-start,1)\n",
    "        hidden = policy.init_hidden(length-start)\n",
    "        inps=policy(ccs,hidden).gather(1,actions)\n",
    "        p1,p2=qvals.detach().max(1)\n",
    "        targ = torch.Tensor(1,p1.shape[0])   \n",
    "        for num in range(start,length,1):\n",
    "            if num==len(store[ep])-1:\n",
    "                targ[0][num-start]=rew[0][num-start] \n",
    "            else:\n",
    "                targ[0][num-start]=rew[0][num-start]+gamma*p1[num-start]\n",
    "        optimizer.zero_grad()\n",
    "        inps=inps.reshape(1,length-start)\n",
    "        loss = criterion(inps,targ)\n",
    "        loss.backward()\n",
    "        for param in policy.parameters():\n",
    "            param.grad.data.clamp(-1,1)\n",
    "        optimizer.step()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainDRQN(episodes):\n",
    "    steps_done=0\n",
    "    avg=0\n",
    "    for i in range(0,episodes,1):\n",
    "        if i%100==0 and i!=0:\n",
    "            print(\"Episode\",i,end=\" \")\n",
    "        env.reset()\n",
    "        prev,_,_,_=env.step(0)\n",
    "        prev=torch.from_numpy(prev)\n",
    "        prev=prev.reshape(1,1,4)\n",
    "        prev=prev.type('torch.FloatTensor')\n",
    "        done=False\n",
    "        steps=0\n",
    "        rew=0\n",
    "        while done == False:\n",
    "            eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "            math.exp(-1. * steps_done / EPS_DECAY)\n",
    "            steps+=1\n",
    "            hidden = policy.init_hidden(1)\n",
    "            output=policy(prev,hidden)\n",
    "            action=(output.argmax()).item()\n",
    "            rand= random.uniform(0,1)\n",
    "            if rand < eps_threshold:\n",
    "                action=random.randint(0,1)\n",
    "            x = env.step(action)\n",
    "            sc,reward,done,_=x\n",
    "            avg=avg+reward\n",
    "            sc=torch.from_numpy(sc)\n",
    "            sc=sc.reshape(1,1,4)\n",
    "            sc=sc.type('torch.FloatTensor')\n",
    "            rew=rew+reward\n",
    "            addEpisode(i,prev.unsqueeze(0),sc.unsqueeze(0),reward,action)\n",
    "            trainNet(i)\n",
    "            prev=sc\n",
    "            steps_done+=1\n",
    "        terminal = torch.zeros(prev.shape[0],prev.shape[1],prev.shape[2])\n",
    "        addEpisode(i,prev.unsqueeze(0),terminal.unsqueeze(0),-10,action)\n",
    "        if i%10==0:\n",
    "            target_net.load_state_dict(policy.state_dict())\n",
    "        if i%100==0 and i!=0:\n",
    "            print(\" -> Average reward over last 100 episodes \" ,avg/100)\n",
    "            avg=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100  -> Average reward over last 100 episodes  20.74\n",
      "Episode 200  -> Average reward over last 100 episodes  57.98\n",
      "Episode 300  -> Average reward over last 100 episodes  114.74\n",
      "Episode 400  -> Average reward over last 100 episodes  105.28\n",
      "Episode 500  -> Average reward over last 100 episodes  125.47\n",
      "Episode 600  -> Average reward over last 100 episodes  126.84\n",
      "Episode 700  -> Average reward over last 100 episodes  121.92\n",
      "Episode 800  -> Average reward over last 100 episodes  136.72\n",
      "Episode 900  -> Average reward over last 100 episodes  72.21\n"
     ]
    }
   ],
   "source": [
    "trainDRQN(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
