{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                            Little Mario Game - Using Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Chapter 21 - Reinforcement Learning - Q-learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Notebook we will make a simple Mario game and solve it using the Q-learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"mario.jpg\" alt=\"Drawing\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Mario game won't be as complex as the actual Mario game. It will look something like this -\n",
    "\n",
    "<img src=\"mario_game.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "So, in this game Little Mario has to cross all the obstacles to reach the Winning cell that is the 10th position. When Little Mario reaches 2nd cell he needs to jump on to the 3rd cell and similarly for 5th and 8th cell to win the game.\n",
    "\n",
    "So, the possible actions that he can take is -\n",
    "0. MOVE STRAIGHT\n",
    "1. JUMP UP\n",
    "\n",
    "The state that he receive after taking an action is just the cell number.\n",
    "\n",
    "Let's define reward now -\n",
    "1. When need to jump -\n",
    "    a) If jumps then - Reward is -0.04 and game continues.\n",
    "    b) No jump - Reward is -1 and game is over.\n",
    "2. When need to go straight -\n",
    "    a) If goes staright - Reward is -0.04 and game continues\n",
    "    b) If jumps - Reward is -0.1 ( To make sure that the agent does not jump unnecessarily)          and game continues.\n",
    "3. On reaching 10th state or Goal cell - Reward is +1 and game is over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "''' This class contains the method get_state_and_reward() which returns whether \n",
    "    the game is over or not, next state and reward received.\n",
    "'''\n",
    "class Mario:\n",
    "    def __init__(self):\n",
    "        self.obstacles = [3,5,8]\n",
    "        self.terminal_state = 10\n",
    "        \n",
    "    \n",
    "    def get_state_and_reward(self,state,action):\n",
    "        \n",
    "        need_to_jump = False\n",
    "        \n",
    "        for i in range(len(self.obstacles)):\n",
    "            if state == self.obstacles[i]-1:\n",
    "                need_to_jump = True\n",
    "                break\n",
    "        \n",
    "        if need_to_jump == True:\n",
    "            if action == 1:\n",
    "                return False,state+1,-0.04\n",
    "            \n",
    "            else:\n",
    "                return True,state+1,-1\n",
    "        elif state == self.terminal_state-1:\n",
    "            return True,state+1,1\n",
    "        else:\n",
    "            if action == 0:\n",
    "                return False,state+1,-0.04\n",
    "            else:\n",
    "                return False,state+1,-0.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning Algorithm\n",
    "\n",
    "In the Q-learning algorithm, we learn the Q-value for the actions taken from a state. Q-value of an action is basically the expected future reward we can get if that action is taken from the current state.\n",
    "\n",
    "Q(st,at)=E[Rt+1+γ∗Rt+2+γ2∗Rt+3+...|(st,at)]\n",
    "\n",
    "Here γ\n",
    "is the discount factor and \n",
    "\n",
    "Rt+1 is the Reward at time step t+1 and so on.\n",
    "\n",
    "The Q-function takes two inputs state and action and returns the expected future reward. In this algorithm, we experience the environment again and again like playing the game several times, every time an action is taken we update its Q-value which was set randomly initially. The update is performed according to the following equation :\n",
    "\n",
    "Q(St,At)=Q(St,At)+α×[R+γ×maxaQ(S′,a)−Q(St,At)]\n",
    "\n",
    "Here α\n",
    "is the learning rate and γ is the discount factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "First we will initialize the Q-table\n",
    "In this implementation QValues[i][0] -> Q-value for the action Going Straight from state i.\n",
    "                       QValues[i][1] -> Q-value for the action Jumping from state i.\n",
    "\n",
    "'''\n",
    "\n",
    "QValues = [[0,0] for i in range(11)]\n",
    "alpha=0.2\n",
    "epsilon=0.2\n",
    "discount=0.9\n",
    "game = Mario()\n",
    "for i in range(1,11,1):\n",
    "    QValues[i][0]=random.uniform(0,1)\n",
    "    QValues[i][1]=random.uniform(0,1)\n",
    "    print('Initial Q-values for state',i,'is: ',\"{0:.3f}\".format(round(QValues[i][0],3)),\"{0:.3f}\".format(round(QValues[i][1],3)))\n",
    "    \n",
    "#Playing the game numgames times\n",
    "numgames = 1000\n",
    "for i in range(numgames):\n",
    "    state = 0\n",
    "    done = False\n",
    "    while done!=True:\n",
    "        num = random.uniform(0,1)\n",
    "        action = 0\n",
    "        if num<epsilon:\n",
    "            action = random.randint(0,1)\n",
    "        else:\n",
    "            action= (QValues[state].index(max(QValues[state])))\n",
    "        done,next_state,reward=game.get_state_and_reward(state,action)\n",
    "        if next_state == 10:\n",
    "            continue\n",
    "        else:\n",
    "            nxtlist = QValues[next_state]\n",
    "            currval = QValues[state][action]\n",
    "            QValues[state][action]=currval +  alpha * ( reward + discount*(max(nxtlist)) - currval)\n",
    "        state=next_state\n",
    "            \n",
    "    \n",
    "print()    \n",
    "print('Q-values after playing 1000 games')\n",
    "for i in range(1,11,1):\n",
    "    print('Q-values for state',i,'is: ',\"{0:.3f}\".format(round(QValues[i][0],3)),\"{0:.3f}\".format(round(QValues[i][1],3)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that after playing for 1000 games, the Q-value corresponding to the states 2,4 and 7 is high for Jumping. Whereas for other states going straight has slightly higher value than jumping.\n",
    "\n",
    "Feel free to change the obstacles position to any other position and even increase the number of time steps from 10 and observe the Q-values of different states.\n",
    "\n",
    "Go ahead and try the Practise Notebook for a little more involved environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
