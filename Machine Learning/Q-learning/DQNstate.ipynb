{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import gym\n",
    "import numpy as np\n",
    "import collections \n",
    "import PIL\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "import matplotlib.cm as cm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(model,self).__init__()\n",
    "        self.fc1= nn.Linear(8,100)\n",
    "        self.fc2 = nn.Linear(100, 3)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=x.reshape(x.shape[0],1,8)\n",
    "        x=self.fc1(x)\n",
    "        return self.fc2(x.contiguous().view(x.size(0), -1))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.5107029,  0.       ])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0').unwrapped\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy=model()\n",
    "target_net=model()\n",
    "target_net.load_state_dict(policy.state_dict())\n",
    "target_net.eval()\n",
    "optimizer = optim.RMSprop(policy.parameters())\n",
    "criterion = F.smooth_l1_loss\n",
    "memory=10000\n",
    "store=[[dict()] for i in range(memory)]\n",
    "gamma=0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addEpisode(ind,prev,curr,reward,act):\n",
    "    if len(store[ind]) ==0:\n",
    "        store[ind][0]={'prev':prev,'curr':curr,'reward':reward,'action':act}\n",
    "    else:\n",
    "        store[ind].append({'prev':prev,'curr':curr,'reward':reward,'action':act})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StackStates(queue):\n",
    "    stack = np.zeros((8))\n",
    "    stack[:2]=queue[0]\n",
    "    stack[2:4]=queue[1]\n",
    "    stack[4:6]=queue[2]\n",
    "    stack[6:8]=queue[3]\n",
    "    return stack\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNet(total_episodes):\n",
    "    if total_episodes==0:\n",
    "        return\n",
    "    ep=random.randint(0,total_episodes-1)\n",
    "    if len(store[ep]) < 8:\n",
    "        return\n",
    "    else:  \n",
    "        start=random.randint(1,len(store[ep])-8)\n",
    "        length=len(store[ep])\n",
    "        inp=[]\n",
    "        target=[]\n",
    "        rew=torch.Tensor(1,8)\n",
    "        actions=torch.Tensor(1,8)\n",
    "        \n",
    "        for i in range(start,start+8,1):\n",
    "            inp.append((store[ep][i]).get('prev'))\n",
    "            target.append((store[ep][i]).get('curr'))\n",
    "            rew[0][i-start]=store[ep][i].get('reward')\n",
    "            actions[0][i-start]=store[ep][i].get('action')\n",
    "        targets = torch.Tensor(target[0].shape[0],target[0].shape[1],target[0].shape[2])\n",
    "        torch.cat(target, out=targets)\n",
    "        ccs=torch.Tensor(inp[0].shape[0],inp[0].shape[1],inp[0].shape[2])\n",
    "        torch.cat(inp, out=ccs)\n",
    "        qvals= target_net(targets)\n",
    "        actions=actions.type('torch.LongTensor')\n",
    "        actions=actions.reshape(8,1)\n",
    "        inps=policy(ccs).gather(1,actions)\n",
    "        p1,p2=qvals.detach().max(1)\n",
    "        targ = torch.Tensor(1,p1.shape[0])   \n",
    "        for num in range(start,start+8,1):\n",
    "            if num==len(store[ep])-1:\n",
    "                targ[0][num-start]=rew[0][num-start] \n",
    "            else:\n",
    "                targ[0][num-start]=rew[0][num-start]+gamma*p1[num-start]\n",
    "        optimizer.zero_grad()\n",
    "        inps=inps.reshape(1,8)\n",
    "        loss = criterion(inps,targ)\n",
    "        loss.backward()\n",
    "        for param in policy.parameters():\n",
    "            param.grad.data.clamp(-1,1)\n",
    "        optimizer.step()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainDQN(episodes):\n",
    "    steps_done=0\n",
    "    avg=0\n",
    "    for i in range(0,episodes,1):\n",
    "        if i%100==0 and i!=0:\n",
    "            print(\"Episode\",i,end=\" \")\n",
    "        env.reset()\n",
    "        queue=collections.deque()\n",
    "        for j in range(4):\n",
    "            state,_,_,_=env.step(random.randint(0,2))\n",
    "            queue.append(state)\n",
    "        prev = StackStates(queue)\n",
    "        prev=torch.from_numpy(prev)\n",
    "        prev=prev.reshape(1,1,8)\n",
    "        prev=prev.type('torch.FloatTensor')\n",
    "        done=False\n",
    "        steps=0\n",
    "        rew=0\n",
    "        moves=0\n",
    "        while moves<200:\n",
    "            moves+=1\n",
    "            eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "            math.exp(-1. * steps_done / EPS_DECAY)\n",
    "            steps+=1\n",
    "            output=policy(prev)\n",
    "            action=(output.argmax()).item()\n",
    "            rand= random.uniform(0,1)\n",
    "            if rand < eps_threshold:\n",
    "                action=random.randint(0,2)\n",
    "            x=env.step(action)\n",
    "            sc,reward,done,_=x\n",
    "            avg=avg+reward\n",
    "            queue.popleft()\n",
    "            queue.append(sc)\n",
    "            sc = StackStates(queue)\n",
    "            sc=torch.from_numpy(sc)\n",
    "            sc=sc.reshape(1,1,8)\n",
    "            sc=sc.type('torch.FloatTensor')\n",
    "            rew=rew+reward\n",
    "            addEpisode(i,prev.unsqueeze(0),sc.unsqueeze(0),reward,action)\n",
    "            trainNet(i)\n",
    "            prev=sc\n",
    "            steps_done+=1\n",
    "        terminal = torch.zeros(prev.shape[0],prev.shape[1],prev.shape[2])\n",
    "        addEpisode(i,prev.unsqueeze(0),terminal.unsqueeze(0),-10,action)\n",
    "        if i%10==0:\n",
    "            target_net.load_state_dict(policy.state_dict())\n",
    "        if i%100==0 and i!=0:\n",
    "            print(\" -> Average reward over last 100 episodes \" ,avg/100)\n",
    "            avg=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100  -> Average reward over last 100 episodes  -202.0\n",
      "Episode 200  -> Average reward over last 100 episodes  -200.0\n",
      "Episode 300  -> Average reward over last 100 episodes  -200.0\n",
      "Episode 400  -> Average reward over last 100 episodes  -200.0\n",
      "Episode 500  -> Average reward over last 100 episodes  -200.0\n",
      "Episode 600  -> Average reward over last 100 episodes  -200.0\n",
      "Episode 700  -> Average reward over last 100 episodes  -200.0\n",
      "Episode 800  -> Average reward over last 100 episodes  -200.0\n",
      "Episode 900  -> Average reward over last 100 episodes  -200.0\n",
      "Episode 1000  -> Average reward over last 100 episodes  -200.0\n",
      "Episode 1100  -> Average reward over last 100 episodes  -200.0\n",
      "Episode 1200  -> Average reward over last 100 episodes  -200.0\n",
      "Episode 1300  -> Average reward over last 100 episodes  -200.0\n",
      "Episode 1400  -> Average reward over last 100 episodes  -200.0\n",
      "Episode 1500  -> Average reward over last 100 episodes  -200.0\n",
      "Episode 1600  -> Average reward over last 100 episodes  -200.0\n",
      "Episode 1700  -> Average reward over last 100 episodes  -200.0\n",
      "Episode 1800  -> Average reward over last 100 episodes  -200.0\n",
      "Episode 1900  -> Average reward over last 100 episodes  -200.0\n"
     ]
    }
   ],
   "source": [
    "trainDQN(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
